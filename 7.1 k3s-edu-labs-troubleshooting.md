# k3s 교육 실습 랩: Pod 장애 분석 / 서비스 라우팅 / HPA / Ingress(Traefik)
> 대상: k3s 클러스터(기본 Traefik), 교육/실습용  
> 목표: **일부러 문제를 만들고 → 진단 → 해결**까지 “손에 익는” 흐름으로 반복

---

## 0) 실습 공통 준비

### 0-1. 네임스페이스 생성
```bash
kubectl create ns demo
```
(이미 있으면 에러 나도 무시 OK)

### 0-2. 기본 앱 배포(nginx) + 서비스
```bash
kubectl -n demo create deploy web --image=nginx:1.27-alpine
kubectl -n demo expose deploy web --port=80 --target-port=80 --name=web-svc
kubectl -n demo get deploy,pod,svc -o wide
```

### 0-3. 편의 변수(선택)
```bash
NS=demo
APP=web
SVC=web-svc
```

---

## 1) LAB-01: Service 라우팅 장애(Endpoints 비어있음) 만들기 & 해결하기
> 핵심 학습: **Service selector ↔ Pod label 불일치** → Endpoints 없음 → 접속 실패

### 1-A. 정상 상태 확인(Endpoints가 존재해야 함)
```bash
kubectl -n demo get svc,ep -o wide
kubectl -n demo get ep web-svc -o yaml | egrep -n 'subsets|addresses|ports'
```

### 1-B. 장애 만들기: Service selector를 일부러 틀리게 바꾸기
아래 YAML로 **selector를 app=wrong**으로 변경합니다.
```bash
cat <<'YAML' | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: web-svc
  namespace: demo
spec:
  selector:
    app: wrong
  ports:
    - name: http
      port: 80
      targetPort: 80
YAML
```

### 1-C. 증상 확인
```bash
kubectl -n demo get svc,ep -o wide
kubectl -n demo get ep web-svc -o yaml | egrep -n 'subsets|addresses|ports'
```
- 기대 결과: **Endpoints가 비어 있음**

클러스터 내부에서 curl 테스트(실패해야 정상)
```bash
kubectl run -n demo tmp --rm -it --image=curlimages/curl -- sh -lc \
"curl -sv http://web-svc:80/ 2>&1 | head -n 40"
```

### 1-D. 원인 진단(정답 루트)
```bash
kubectl -n demo get svc web-svc -o jsonpath='{.spec.selector}'; echo
kubectl -n demo get pod --show-labels
```

### 1-E. 해결: Service selector를 원래대로(app=web) 복구
```bash
cat <<'YAML' | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: web-svc
  namespace: demo
spec:
  selector:
    app: web
  ports:
    - name: http
      port: 80
      targetPort: 80
YAML
```

### 1-F. 해결 검증
```bash
kubectl -n demo get ep web-svc -o wide
kubectl run -n demo tmp --rm -it --image=curlimages/curl -- sh -lc \
"curl -sS http://web-svc:80/ | head"
```

---

## 2) LAB-02: Pod CrashLoopBackOff 만들기 & 해결하기
> 핵심 학습: **describe 이벤트 → logs/previous → exec로 내부 확인**

### 2-A. 장애 만들기: 존재하지 않는 명령으로 컨테이너 시작
(컨테이너가 바로 죽어서 CrashLoopBackOff 발생)
```bash
cat <<'YAML' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: crash-app
  namespace: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: crash-app
  template:
    metadata:
      labels:
        app: crash-app
    spec:
      containers:
        - name: app
          image: busybox:1.36
          command: ["sh","-lc","not-a-real-command"]
YAML
```

### 2-B. 증상 확인
```bash
kubectl -n demo get pod -l app=crash-app -w
```
(잠깐 기다리면 STATUS가 CrashLoopBackOff로 변함)

### 2-C. 원인 진단 루틴(실전 순서)
1) Events(가장 빠름)
```bash
POD=$(kubectl -n demo get pod -l app=crash-app -o jsonpath='{.items[0].metadata.name}')
kubectl -n demo describe pod "$POD" | egrep -n 'Events|Warning|Back-off|Failed|Error'
```

2) logs / previous
```bash
kubectl -n demo logs "$POD" --tail=200
kubectl -n demo logs "$POD" --previous --tail=200
```

### 2-D. 해결: 정상 command로 변경(무한 대기)
```bash
cat <<'YAML' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: crash-app
  namespace: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: crash-app
  template:
    metadata:
      labels:
        app: crash-app
    spec:
      containers:
        - name: app
          image: busybox:1.36
          command: ["sh","-lc","echo ok; sleep 3600"]
YAML
```

### 2-E. 해결 검증
```bash
kubectl -n demo get pod -l app=crash-app -w
kubectl -n demo logs -l app=crash-app --tail=50
```

---

## 3) LAB-03: HPA 실습(사전 조건 점검 → 부하 → 스케일)
> 주의: k3s는 환경에 따라 metrics-server가 없을 수 있습니다.  
> **HPA/`kubectl top`이 안 되면 먼저 metrics부터.**

### 3-A. metrics API 동작 확인
```bash
kubectl get apiservices | grep metrics
kubectl top node 2>&1 | head
```
- `kubectl top`이 에러면: metrics-server가 없거나 동작 문제가 있을 수 있음

#### (선택) metrics-server 존재/로그 점검
```bash
kubectl -n kube-system get deploy | grep metrics
kubectl -n kube-system logs deploy/metrics-server --tail=200 | egrep -i 'error|fail|x509|timeout'
```

### 3-B. HPA 대상 앱 준비(요청량 requests 설정 포함)
```bash
cat <<'YAML' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hpa-app
  namespace: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hpa-app
  template:
    metadata:
      labels:
        app: hpa-app
    spec:
      containers:
      - name: app
        image: nginx:1.27-alpine
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: "50m"
            memory: "64Mi"
          limits:
            cpu: "200m"
            memory: "128Mi"
---
apiVersion: v1
kind: Service
metadata:
  name: hpa-svc
  namespace: demo
spec:
  selector:
    app: hpa-app
  ports:
  - port: 80
    targetPort: 80
YAML
```

### 3-C. HPA 생성(예: CPU 50% 목표)
```bash
kubectl -n demo autoscale deploy hpa-app --cpu-percent=50 --min=1 --max=5
kubectl -n demo get hpa
```

### 3-D. 부하 생성(내부에서 반복 요청)
```bash
kubectl run -n demo load --rm -it --image=busybox:1.36 -- sh -lc \
"while true; do wget -qO- http://hpa-svc:80/ >/dev/null; done"
```

다른 터미널에서 스케일 추적
```bash
kubectl -n demo get hpa -w
kubectl -n demo get pod -l app=hpa-app -w
```

### 3-E. 해석 포인트(교육용)
- **requests가 없으면** CPU 기반 HPA가 이상하게 동작/불가능할 수 있음
- 스케일은 즉시가 아니라 metrics 수집 주기/안정화에 따라 지연될 수 있음
- 부하를 끊으면 scale down은 더 느리게(안정화) 일어나는 게 일반적

---

## 4) LAB-04: Ingress(Traefik) 404/라우팅 문제 만들기 & 해결하기
> k3s 기본 Traefik 가정  
> 핵심 학습: **Host 헤더 기반 라우팅**, **Ingress → Service → Endpoints**로 내려가는 디버깅

### 4-A. Traefik 동작 확인
```bash
kubectl -n kube-system get pod | grep traefik
kubectl -n kube-system get svc | grep traefik
```

### 4-B. (사전) Ingress가 가리킬 서비스 준비
LAB-00에서 만든 `web-svc`를 사용합니다.
```bash
kubectl -n demo get svc web-svc -o wide
```

### 4-C. 장애 만들기: Host를 지정해놓고 Host 헤더 없이 접속하기(→ 404)
Ingress 생성(Host: `demo.local`)
```bash
cat <<'YAML' | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-ing
  namespace: demo
spec:
  rules:
  - host: demo.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-svc
            port:
              number: 80
YAML
```

Ingress 확인
```bash
kubectl -n demo get ingress
kubectl -n demo describe ingress web-ing | egrep -n 'Rules|Host|Path|Backend|Events'
```

### 4-D. Traefik 서비스 노출 방식 확인(NodePort가 흔함)
```bash
kubectl -n kube-system get svc traefik -o wide
kubectl -n kube-system describe svc traefik | egrep -n 'Type:|NodePort|LoadBalancer|Ports'
```

> 아래에서 `<NODEIP>`는 노드 IP(예: k3s 서버 IP), `<NODEPORT>`는 traefik의 80 포트 NodePort를 넣으세요.

### 4-E. “Host 헤더 없이” 접속(의도적으로 404가 나오는지 확인)
```bash
curl -i "http://<NODEIP>:<NODEPORT>/" | head -n 30
```

### 4-F. 해결: Host 헤더를 넣어 접속(정상 라우팅)
```bash
curl -i -H "Host: demo.local" "http://<NODEIP>:<NODEPORT>/" | head -n 30
```

### 4-G. 502/504가 뜬다면(백엔드 문제) → 아래로 내려가며 점검
1) Service/Endpoints
```bash
kubectl -n demo get svc,ep -o wide
```
2) Pod 상태/Readiness
```bash
kubectl -n demo get pod -o wide
kubectl -n demo describe pod -l app=web | egrep -n 'Ready|Readiness|probe|Events'
```
3) Traefik 로그(힌트 찾기)
```bash
kubectl -n kube-system logs -l app.kubernetes.io/name=traefik --tail=200 | egrep -i 'error|warn|service|router|timeout'
```

---

## 5) 정리: “K8s vs Linux” 구분 포인트(실습 관찰용)

- `kubectl get/describe/logs/exec/apply` → **K8s API(클러스터 리소스)**
- `| grep/egrep/awk/sort/head/tail` → **Linux 쉘(출력 텍스트 가공)**
- `kubectl exec POD -- <cmd>` 의 `<cmd>` → **Pod 내부 Linux**
- `curl`/`wget` → **네트워크 테스트(Linux)**  
  - Ingress는 특히 `-H "Host: ..."`가 핵심

---

## 6) 실습 리셋/정리(원할 때)
```bash
kubectl delete ns demo
```
(전부 삭제 후 재시작)

